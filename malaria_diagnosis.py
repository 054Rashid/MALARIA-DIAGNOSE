# -*- coding: utf-8 -*-
"""Malaria diagnosis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ovz5ENDL-hnhM3UIdJXT34vsG9ELFJAD
"""

print("="*70)
print("ðŸš€ GOOGLE COLAB SETUP")
print("="*70)

import tensorflow as tf
import tensorflow_datasets as tfds
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
import os
import datetime

from tensorflow.keras import layers, models, callbacks
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import (Conv2D, MaxPooling2D, Dense, Flatten,
                                      Dropout, BatchNormalization, GlobalAveragePooling2D)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.metrics import BinaryAccuracy, Precision, Recall, AUC

# Check Colab
try:
    import google.colab
    IN_COLAB = True
    print("âœ“ Running on Google Colab")
except:
    IN_COLAB = False
    print("âœ“ Running locally")

# Check GPU
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    print(f"âœ“ GPU Available: {len(gpus)} GPU(s)")
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)
else:
    print("âš ï¸  No GPU - Enable GPU in Runtime â†’ Change runtime type")

# Google Drive (optional)
if IN_COLAB:
    print("\n" + "="*70)
    mount_drive = input("Mount Google Drive to save models? (y/n): ").lower()

    if mount_drive == 'y':
        from google.colab import drive
        drive.mount('/content/drive')
        SAVE_DIR = '/content/drive/MyDrive/Malaria_Project/'
        os.makedirs(SAVE_DIR, exist_ok=True)
        os.makedirs(SAVE_DIR + 'models', exist_ok=True)
        os.makedirs(SAVE_DIR + 'visualizations', exist_ok=True)
        print(f"âœ“ Saving to: {SAVE_DIR}")
    else:
        SAVE_DIR = '/content/'
else:
    SAVE_DIR = './'

print("="*70)

# CONFIGURATION - OPTIMIZED FOR HIGH ACCURACY
CONFIG = {
    'IMG_SIZE': 128,        # 128 is optimal (faster + good accuracy)
    'BATCH_SIZE': 32,       # Balanced batch size
    'EPOCHS': 20,           # Enough for convergence
    'INITIAL_LR': 0.001,    # Higher LR for better learning
    'FINE_TUNE_LR': 0.0001, # Lower LR for fine-tuning
    'TRAIN_SPLIT': 0.8,
    'VAL_SPLIT': 0.1,
    'TEST_SPLIT': 0.1
}

np.random.seed(42)
tf.random.set_seed(42)

# 1. DATA LOADING

print("\n" + "="*50)
print("STEP 1: LOADING DATASET")
print("="*50)

dataset, info = tfds.load(
    'malaria',
    with_info=True,
    as_supervised=True,
    shuffle_files=True,
    split=['train']
)

dataset = dataset[0]
dataset_size = info.splits['train'].num_examples

print(f"\nâœ“ Total samples: {dataset_size}")
print(f"âœ“ Classes: {info.features['label'].names}")

# 2. DATA SPLITTING

print("\n" + "="*50)
print("STEP 2: SPLITTING DATASET")
print("="*50)

train_size = int(CONFIG['TRAIN_SPLIT'] * dataset_size)
val_size = int(CONFIG['VAL_SPLIT'] * dataset_size)
test_size = dataset_size - train_size - val_size

train_ds = dataset.take(train_size)
val_test_ds = dataset.skip(train_size)
val_ds = val_test_ds.take(val_size)
test_ds = val_test_ds.skip(val_size)

print(f"âœ“ Train: {train_size}")
print(f"âœ“ Validation: {val_size}")
print(f"âœ“ Test: {test_size}")

# 3. DATA PREPROCESSING - BALANCED AUGMENTATION

print("\n" + "="*50)
print("STEP 3: DATA PREPROCESSING")
print("="*50)

def preprocess(image, label):
    """Simple preprocessing for val/test"""
    image = tf.image.resize(image, (CONFIG['IMG_SIZE'], CONFIG['IMG_SIZE']))
    image = image / 255.0
    return image, label

def augment(image, label):
    """BALANCED augmentation - not too aggressive"""
    image = tf.image.resize(image, (CONFIG['IMG_SIZE'], CONFIG['IMG_SIZE']))

    # Random flips (50% chance each)
    image = tf.image.random_flip_left_right(image)
    image = tf.image.random_flip_up_down(image)

    # Mild brightness and contrast
    image = tf.image.random_brightness(image, max_delta=0.1)
    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)

    # Normalize
    image = image / 255.0
    return image, label

# Prepare datasets - PROPER CONFIGURATION
AUTOTUNE = tf.data.AUTOTUNE

train_ds = (train_ds
    .shuffle(1000)
    .map(augment, num_parallel_calls=AUTOTUNE)
    .batch(CONFIG['BATCH_SIZE'])
    .prefetch(AUTOTUNE)
)

val_ds = (val_ds
    .map(preprocess, num_parallel_calls=AUTOTUNE)
    .batch(CONFIG['BATCH_SIZE'])
    .prefetch(AUTOTUNE)
)

test_ds = (test_ds
    .map(preprocess, num_parallel_calls=AUTOTUNE)
    .batch(CONFIG['BATCH_SIZE'])
)

print("âœ“ Data preprocessing complete")
print(f"  Train batches: {len(list(train_ds))}")
print(f"  Val batches: {len(list(val_ds))}")
print(f"  Test batches: {len(list(test_ds))}")

# 4. VISUALIZE SAMPLES

print("\n" + "="*50)
print("STEP 4: VISUALIZING SAMPLES")
print("="*50)

plt.figure(figsize=(10, 10))
for images, labels in train_ds.take(1):
    for i in range(9):
        ax = plt.subplot(3, 3, i + 1)
        plt.imshow(images[i])
        label_name = info.features['label'].int2str(labels[i].numpy())
        plt.title(label_name)
        plt.axis('off')
plt.tight_layout()
plt.savefig(os.path.join(SAVE_DIR, 'visualizations/samples.png'), dpi=100)
plt.show()
print("âœ“ Samples visualized")

# 5. BUILD MODEL - TRANSFER LEARNING

print("\n" + "="*50)
print("STEP 5: BUILDING MODEL")
print("="*50)

# Load MobileNetV2
base_model = MobileNetV2(
    input_shape=(CONFIG['IMG_SIZE'], CONFIG['IMG_SIZE'], 3),
    include_top=False,
    weights='imagenet'
)

# Freeze base model
base_model.trainable = False

# Build model
model = models.Sequential([
    base_model,
    GlobalAveragePooling2D(),

    Dense(256, activation='relu'),
    BatchNormalization(),
    Dropout(0.5),

    Dense(128, activation='relu'),
    BatchNormalization(),
    Dropout(0.3),

    Dense(1, activation='sigmoid')
], name='Malaria_Detector')

model.summary()
print(f"\n Total parameters: {model.count_params():,}")

# 6. COMPILE MODEL

print("\n" + "="*50)
print("STEP 6: COMPILING MODEL")
print("="*50)

metrics = [
    BinaryAccuracy(name='accuracy'),
    Precision(name='precision'),
    Recall(name='recall'),
    AUC(name='auc')
]

model.compile(
    optimizer=Adam(learning_rate=CONFIG['INITIAL_LR']),
    loss=BinaryCrossentropy(),
    metrics=metrics
)

print(" Model compiled")

# 7. CALLBACKS

print("\n" + "="*50)
print("STEP 7: SETTING UP CALLBACKS")
print("="*50)

checkpoint_cb = callbacks.ModelCheckpoint(
    os.path.join(SAVE_DIR, 'models/best_model.h5'),
    monitor='val_accuracy',
    save_best_only=True,
    mode='max',
    verbose=1
)

early_stop_cb = callbacks.EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True,
    verbose=1
)

reduce_lr_cb = callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=3,
    min_lr=1e-7,
    verbose=1
)

print(" Callbacks configured")

# 8. STAGE 1 TRAINING - FROZEN BASE

print("\n" + "="*50)
print("STEP 8: STAGE 1 TRAINING (Frozen Base)")
print("="*50)
print("Training top layers only (10 epochs)...")

history1 = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=10,
    callbacks=[checkpoint_cb, early_stop_cb, reduce_lr_cb],
    verbose=1
)

print("\n Stage 1 complete!")
print(f"  Best val accuracy: {max(history1.history['val_accuracy'])*100:.2f}%")

# 9. STAGE 2 TRAINING - FINE-TUNING

print("\n" + "="*50)
print("STEP 9: STAGE 2 TRAINING (Fine-Tuning)")
print("="*50)

# Unfreeze base model
base_model.trainable = True

# Freeze first 100 layers
for layer in base_model.layers[:100]:
    layer.trainable = False

print(f" Unfroze last {len(base_model.layers) - 100} layers")

# Recompile with lower LR
model.compile(
    optimizer=Adam(learning_rate=CONFIG['FINE_TUNE_LR']),
    loss=BinaryCrossentropy(),
    metrics=metrics
)

print(" Recompiled for fine-tuning")

# Continue training
history2 = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=CONFIG['EPOCHS'],
    initial_epoch=len(history1.history['loss']),
    callbacks=[checkpoint_cb, early_stop_cb, reduce_lr_cb],
    verbose=1
)

print("\n Stage 2 complete!")
print(f"  Best val accuracy: {max(history2.history['val_accuracy'])*100:.2f}%")

# Combine histories
history_combined = {}
for key in history1.history.keys():
    history_combined[key] = history1.history[key] + history2.history[key]

# 10. PLOT TRAINING HISTORY

print("\n" + "="*50)
print("STEP 10: PLOTTING TRAINING HISTORY")
print("="*50)

fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Accuracy
axes[0, 0].plot(history_combined['accuracy'], label='Train', linewidth=2)
axes[0, 0].plot(history_combined['val_accuracy'], label='Val', linewidth=2)
axes[0, 0].axvline(x=10, color='red', linestyle='--', label='Fine-tuning starts')
axes[0, 0].set_title('Accuracy', fontsize=14, fontweight='bold')
axes[0, 0].set_xlabel('Epoch')
axes[0, 0].set_ylabel('Accuracy')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# Loss
axes[0, 1].plot(history_combined['loss'], label='Train', linewidth=2)
axes[0, 1].plot(history_combined['val_loss'], label='Val', linewidth=2)
axes[0, 1].axvline(x=10, color='red', linestyle='--', label='Fine-tuning starts')
axes[0, 1].set_title('Loss', fontsize=14, fontweight='bold')
axes[0, 1].set_xlabel('Epoch')
axes[0, 1].set_ylabel('Loss')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

# Precision
axes[1, 0].plot(history_combined['precision'], label='Train', linewidth=2)
axes[1, 0].plot(history_combined['val_precision'], label='Val', linewidth=2)
axes[1, 0].axvline(x=10, color='red', linestyle='--', label='Fine-tuning starts')
axes[1, 0].set_title('Precision', fontsize=14, fontweight='bold')
axes[1, 0].set_xlabel('Epoch')
axes[1, 0].set_ylabel('Precision')
axes[1, 0].legend()
axes[1, 0].grid(True, alpha=0.3)

# Recall
axes[1, 1].plot(history_combined['recall'], label='Train', linewidth=2)
axes[1, 1].plot(history_combined['val_recall'], label='Val', linewidth=2)
axes[1, 1].axvline(x=10, color='red', linestyle='--', label='Fine-tuning starts')
axes[1, 1].set_title('Recall', fontsize=14, fontweight='bold')
axes[1, 1].set_xlabel('Epoch')
axes[1, 1].set_ylabel('Recall')
axes[1, 1].legend()
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(os.path.join(SAVE_DIR, 'visualizations/training_history.png'), dpi=150)
plt.show()
print("âœ“ Training history saved")

# 11. EVALUATE ON TEST SET

print("\n" + "="*50)
print("STEP 11: EVALUATING ON TEST SET")
print("="*50)

# Load best model
model = tf.keras.models.load_model(os.path.join(SAVE_DIR, 'models/best_model.h5'))

test_results = model.evaluate(test_ds, verbose=1)

print("\n" + "="*70)
print("ðŸŽ¯ FINAL TEST RESULTS")
print("="*70)
print(f"  Loss: {test_results[0]:.4f}")
print(f"  âœ“ Accuracy: {test_results[1]*100:.2f}%")
print(f"  âœ“ Precision: {test_results[2]*100:.2f}%")
print(f"  âœ“ Recall: {test_results[3]*100:.2f}%")
print(f"  âœ“ AUC: {test_results[4]:.4f}")
print("="*70)

# 12. CONFUSION MATRIX

print("\n" + "="*50)
print("STEP 12: CONFUSION MATRIX")
print("="*50)

y_true = []
y_pred = []

for images, labels in test_ds:
    predictions = model.predict(images, verbose=0)
    y_true.extend(labels.numpy())
    y_pred.extend(predictions.flatten())

y_pred_binary = (np.array(y_pred) > 0.5).astype(int)

cm = confusion_matrix(y_true, y_pred_binary)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Parasitized', 'Uninfected'],
            yticklabels=['Parasitized', 'Uninfected'])
plt.title('Confusion Matrix', fontsize=16, fontweight='bold')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.tight_layout()
plt.savefig(os.path.join(SAVE_DIR, 'visualizations/confusion_matrix.png'), dpi=150)
plt.show()

print("\n" + classification_report(y_true, y_pred_binary,
                                   target_names=['Parasitized', 'Uninfected']))

# 13. ROC CURVE

print("\n" + "="*50)
print("STEP 13: ROC CURVE")
print("="*50)

fpr, tpr, _ = roc_curve(y_true, y_pred)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve', fontsize=14, fontweight='bold')
plt.legend(loc="lower right")
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig(os.path.join(SAVE_DIR, 'visualizations/roc_curve.png'), dpi=150)
plt.show()

# 14. VISUALIZE PREDICTIONS

print("\n" + "="*50)
print("STEP 14: VISUALIZING PREDICTIONS")
print("="*50)

plt.figure(figsize=(12, 12))
for images, labels in test_ds.take(1):
    predictions = model.predict(images, verbose=0)

    for i in range(min(16, len(images))):
        ax = plt.subplot(4, 4, i + 1)
        plt.imshow(images[i])

        actual = 'Parasitized' if labels[i] == 0 else 'Uninfected'
        pred_prob = predictions[i][0]
        predicted = 'Parasitized' if pred_prob < 0.5 else 'Uninfected'

        color = 'green' if actual == predicted else 'red'
        confidence = (1 - pred_prob) * 100 if pred_prob < 0.5 else pred_prob * 100

        plt.title(f'True: {actual}\nPred: {predicted}\n{confidence:.1f}%',
                 color=color, fontsize=8, fontweight='bold')
        plt.axis('off')

plt.tight_layout()
plt.savefig(os.path.join(SAVE_DIR, 'visualizations/predictions.png'), dpi=150)
plt.show()

# 15. SAVE FINAL MODEL

print("\n" + "="*50)
print("STEP 15: SAVING FINAL MODEL")
print("="*50)

final_model_path = os.path.join(SAVE_DIR, 'models/malaria_detector_final.h5')
model.save(final_model_path)
print(f"âœ“ Model saved: {final_model_path}")

# 16. FINAL SUMMARY

print("\n" + "="*70)
print("PROJECT SUMMARY")
print("="*70)

print("\n FINAL METRICS:")
print(f"  Accuracy: {test_results[1]*100:.2f}%")
print(f"  Precision: {test_results[2]*100:.2f}%")
print(f"  Recall: {test_results[3]*100:.2f}%")
print(f"  AUC: {test_results[4]:.4f}")

print("\n TECHNIQUES USED:")
print(" Transfer Learning (MobileNetV2)")
print(" Two-Stage Training")
print(" Balanced Data Augmentation")
print(" Learning Rate Scheduling")
print(" Early Stopping")

print("\n FILES SAVED:")
print(f"  {os.path.join(SAVE_DIR, 'models/best_model.h5')}")
print(f"  {os.path.join(SAVE_DIR, 'models/malaria_detector_final.h5')}")
print(f"  {os.path.join(SAVE_DIR, 'visualizations/*.png')}")

print("\n" + "="*70)
if test_results[1] >= 0.96:
    print(" SUCCESS! 96%+ ACCURACY ACHIEVED!")
elif test_results[1] >= 0.93:
    print("EXCELLENT! 93%+ ACCURACY ACHIEVED!")
else:
    print(" Good progress! For higher accuracy:")
    print("  1. Train for more epochs (increase CONFIG['EPOCHS'])")
    print("  2. Use larger image size (CONFIG['IMG_SIZE'] = 224)")
    print("  3. Ensure GPU is enabled for faster training")
print("="*70)

print("""
  TO USE THE MODEL:

model = tf.keras.models.load_model('{}')

def predict(image_path):
    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(128, 128))
    img_array = tf.keras.preprocessing.image.img_to_array(img) / 255.0
    img_array = np.expand_dims(img_array, axis=0)
    pred = model.predict(img_array)[0][0]

    if pred < 0.5:
        return f"Parasitized ({{(1-pred)*100:.1f}}% confidence)"
    else:
        return f"Uninfected ({{pred*100:.1f}}% confidence)"
""".format(final_model_path))

